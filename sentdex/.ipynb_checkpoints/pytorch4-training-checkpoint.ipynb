{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "49375755-b5d4-4600-be65-be6a44ee538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO building neural network\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7e4c68d4-68b1-41aa-9863-31135b77f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3134620c-7a03-4448-8764-15dc2ef1456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([64, 784])\n",
      "fc1.bias \t torch.Size([64])\n",
      "fc2.weight \t torch.Size([64, 64])\n",
      "fc2.bias \t torch.Size([64])\n",
      "fc3.weight \t torch.Size([64, 64])\n",
      "fc3.bias \t torch.Size([64])\n",
      "fc4.weight \t torch.Size([10, 64])\n",
      "fc4.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n",
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "# Initialize model\n",
    "net = Net()\n",
    "\n",
    "# Initialize optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e45c5c67-6bd0-40e4-939a-848d8685e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1540, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2752, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0816, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1002e-05, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d6a1935f-a095-4138-bc56-db30d32f140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.991\n"
     ]
    }
   ],
   "source": [
    "# check how good is network\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e7af0c12-d915-439d-a280-943e9dee1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "import time\n",
    "PATH = f\"MNIST_trained_model-{EPOCHS}_epochs-{int(time.time())}\"\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "92d04d40-b8b8-4ba9-b6ff-a1f96a409127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "MODEL_NAME = \"MNIST_trained_model-10_epochs-1718478650\"\n",
    "net.load_state_dict(torch.load(MODEL_NAME))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6f3ab926-fd43-4af2-9fca-9128e164658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "43bef8c7-cbc2-48e9-86ad-600a96772cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1, 28*28))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8479846-1b6b-4d27-a5f2-7fe560e4a173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# recognize custom made digits\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_number = 0\n",
    "while os.path.isfile(f\"digits/digit{image_number}.png\"):\n",
    "    try:\n",
    "        # Load and prepare the image\n",
    "        img = cv2.imread(f\"digits/digit{image_number}.png\")[:,:,0]\n",
    "\n",
    "        # Resizing to prevent misshaped digits\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "\n",
    "        # Perform thresholding to binarize the image\n",
    "        #_, img = cv2.threshold(img, thresh=128, maxval=255, type=cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        img = np.invert(img)\n",
    "        # Convert the prepared image to a PyTorch Tensor\n",
    "        input_tensor = torch.from_numpy(img).float() # convert the data type to float\n",
    "        input_tensor = input_tensor.reshape(-1, 28 * 28)/ 255.0 # faltten tensor into 1D\n",
    "\n",
    "        #print(input_tensor[0])\n",
    "        # Feed the input Tensor to the network and display the prediction\n",
    "        output = net(input_tensor)\n",
    "        print(f\"Output tensor: {output}\")\n",
    "        pred_probs = output.detach().numpy()\n",
    "        pred_class = np.argmax(pred_probs)\n",
    "        print(f\"Predicted probability distribution: {pred_probs}\")\n",
    "        print(f\"{image_number}: Predicted class index: {pred_class}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    image_number += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e5925-4406-472d-ad37-8022529393f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_tensor[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89cf94-8879-412e-b08f-b44664b7573d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1648761-180b-4094-9334-bcb29fd0213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0aa4f-baae-4a01-abb9-d09d5a941923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "bd28e373-fd73-4a6c-ba86-64dcf640ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clasify custom made digits from gradio sketchpad WIP\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def classify_sketch(sketch):\n",
    "    sketch = np.moveaxis(np.array(sketch['layers']), source=-1, destination=0)\n",
    "    if len(sketch.shape) == 4:\n",
    "        sketch = sketch[:3]\n",
    "    sketch = cv2.merge([sketch]*3)  # Ensure sketch has 3 channels\n",
    "\n",
    "    mem_file = io.BytesIO()\n",
    "    rgb_sketch = cv2.cvtColor(sketch, cv2.COLOR_RGBA2BGR)\n",
    "    result, encimg = cv2.imencode(\".png\", rgb_sketch)\n",
    "    encoded_bytes = cv2.imencode(\".png\", rgb_sketch)[1]\n",
    "    memory_buffer = io.BytesIO(encoded_bytes)\n",
    "    memory_buffer.seek(0)\n",
    "\n",
    "    opencv_img = cv2.imdecode(np.frombuffer(memory_buffer.getvalue(), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    pil_img = Image.fromarray(opencv_img)\n",
    "\n",
    "    sketch_filename = f'temp_sketch_{pil_img.width}_{pil_img.height}.png'\n",
    "    pil_img.save(sketch_filename)\n",
    "\n",
    "    processed_sketch = cv2.imread(sketch_filename, cv2.IMREAD_GRAYSCALE)\n",
    "    processed_sketch = cv2.resize(processed_sketch, (28, 28)).flatten() / 255.0\n",
    "    input_tensor = torch.from_numpy(processed_sketch).float().unsqueeze(0)\n",
    "\n",
    "\n",
    "    output = net(input_tensor)\n",
    "    print(f\"Output tensor: {output}\")\n",
    "    pred_probs = output.detach().numpy()\n",
    "    pred_class = np.argmax(pred_probs)\n",
    "    print(f\"Predicted probability distribution: {pred_probs}\")\n",
    "    print(f\"{image_number}: Predicted class index: {pred_class}\\n\")\n",
    "\n",
    "    #os.remove(sketch_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "de973d3d-bb42-4712-abef-09d0b578b02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7949\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7949/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\gradio\\queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\gradio\\blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\gradio\\blocks.py\", line 1514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\miniconda3\\envs\\pytorchenv\\Lib\\site-packages\\gradio\\utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T3500\\AppData\\Local\\Temp\\ipykernel_185636\\1649727739.py\", line 9, in classify_sketch\n",
      "    sketch = cv2.add(sketch, layer['data'][0]['content'])\n",
      "                             ~~~~~^^^^^^^^\n",
      "IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n"
     ]
    }
   ],
   "source": [
    "label = gr.Label(num_top_classes=3)\n",
    "interface = gr.Interface(fn=classify_sketch, inputs=\"sketchpad\", outputs=label)\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebaf88-0617-40a3-a65c-9c45f611397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
